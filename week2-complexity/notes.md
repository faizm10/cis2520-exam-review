# Week 2: Algorithm Complexity and Sorting

## Overview
This week focuses on understanding the performance of algorithms and the efficiency of sorting methods. Key topics include analyzing algorithms, asymptotic notations, and exploring sorting algorithms with their time and space complexities.

---

## Table of Contents
1. [What Are Algorithms?](#what-are-algorithms)
2. [Analyzing Algorithms](#analyzing-algorithms)
3. [Asymptotic Notations](#asymptotic-notations)
4. [Complexity Calculations](#complexity-calculations)
5. [Sorting Algorithms](#sorting-algorithms)
6. [Summary of Sorting Complexities](#summary-of-sorting-complexities)
7. [Resources](#resources)

---

## What Are Algorithms?

### Definition
An **algorithm** is a step-by-step, well-defined computational process that transforms inputs into outputs to solve a specific problem.

### Key Concepts
- **Problem**: The goal to be achieved.
- **Instance**: An example of inputs and outputs.
- **Correct Algorithm**: Produces the correct output for all valid inputs.

#### Example: Sorting Problem
**Input**: A sequence of `n` numbers.  
**Output**: A permutation of the input in ascending order.  
**Example**:  
Input: `[7, 4, 3, 1, 5, 4]` ‚Üí Output: `[1, 3, 4, 4, 5, 7]`

---

## Analyzing Algorithms

### Why Analyze Algorithms?
To predict:
- **Time Complexity**: How long the algorithm takes.
- **Space Complexity**: How much memory the algorithm uses.

### Key Factors
1. **Input Size**: Examples:
   - Sorting: Number of elements/items.
   - Graphs: Number of vertices and edges.
2. **Operations**:
   - Examples: Comparisons, additions, multiplications.

### Analyzing Algorithm -- Operations
- **Best Case**: Fastest performance.
- **Worst Case**: Slowest performance.
- **Average Case**: Performance averaged over all inputs.

#### Example: Loops and Their Growth
1. **Linear Loop**:
   ```c
   for (i = 1; i <= n; i++) {
       x = x + 1;
   }
   // Total operations: O(n)
   ```
2. **Nested Loop**:
   ```c
   for (i = 1; i <= n; i++) {
       for (j = 1; j <= n; j++) {
           x = x + 1;
       }
   }
   // Total operations: O(n^2)
   ```

---

## Asymptotic Notations

### 1. Big-O Notation (`O`)
Defines the **upper bound** of an algorithm's runtime. It describes the worst-case scenario.
- **Formal Definition**: `f(n) ‚àà O(g(n))` if there exist constants `c > 0` and `n0` such that for all `n ‚â• n0`:  
  `f(n) ‚â§ c * g(n)`

- Constant c could depend on
    ‚Äì the programming language used,
    ‚Äì the quality of the compiler or interpreter,
    ‚Äì the CPU speed,
    ‚Äì the size of the main memory and the access time to it,
    ‚Äì the knowledge of the programmer,
    ‚Äì the algorithm itself, which may require simple but also time-consuming machine instructions

### Big-O Examples

#### Example 1: Show that `2n = O(n¬≤)`

To prove `2n = O(n¬≤)`, we do the following steps:

1. By definition, we need to find a constant `c` such that:  
   `f(n) ‚â§ c * g(n)`

2. Substitute `f(n) = 2n` and `g(n) = n¬≤`:  
   `2n ‚â§ c * n¬≤`

3. Simplify by dividing both sides by `n` (valid since `n > 0`):  
   `2 ‚â§ c`

4. Choose constants `c` and `n‚ÇÄ` to satisfy the Big-O definition:  
   `c = 2`, `n‚ÇÄ = 1`

Thus, for `n ‚â• 1`, the inequality holds true. Therefore:  
`2n = O(n¬≤)`


### 2. Omega Notation (`Œ©`)
Defines the **lower bound** of an algorithm's runtime. It describes the best-case scenario.
- **Formal Definition**: `f(n) ‚àà Œ©(g(n))` if there exist constants `c > 0` and `n0` such that for all `n ‚â• n0`:  
  `f(n) ‚â• c * g(n)`

### Omega Notation Example

#### Example: Show that `2n ‚â† Œ©(n¬≤)`

To prove `2n ‚â† Œ©(n¬≤)`, we do the following steps:

1. By definition, we need to find a constant `c` such that:  
   `f(n) ‚â• c * g(n)`

2. Substitute `f(n) = 2n` and `g(n) = n¬≤`:  
   `2n ‚â• c * n¬≤`

3. Simplify by dividing both sides by `n` (valid since `n > 0`):  
   `2/n ‚â• c`

4. As `n` becomes larger, the value of `2/n` gets closer and closer to 0.

5. The only possible value for `c` would be `0`, but `c` must be a positive constant (`c ‚àà R‚Å∫`).

6. Therefore, no such constant `c` exists, and we conclude that:  
   `2n ‚â† Œ©(n¬≤)`

This proves the inequality by contradiction.

### 3. Theta Notation (`Œò`)
Defines a **tight bound**, meaning the algorithm runs between a lower and upper bound.
- **Formal Definition**: `f(n) ‚àà Œò(g(n))` if `f(n) ‚àà O(g(n))` and `f(n) ‚àà Œ©(g(n))`.

### Theta Notation Example

#### Example: Show that `2n = Œò(n)`

To prove `2n = Œò(n)`, we do the following steps:

1. By definition, we need to find constants `c‚ÇÅ` and `c‚ÇÇ` such that:  
   `c‚ÇÅ * g(n) ‚â§ f(n) ‚â§ c‚ÇÇ * g(n)`

2. Substitute `f(n) = 2n` and `g(n) = n`:  
   `c‚ÇÅ * n ‚â§ 2n ‚â§ c‚ÇÇ * n`

3. Divide all terms by `n` (valid since `n > 0`):  
   `c‚ÇÅ ‚â§ 2 ‚â§ c‚ÇÇ`

4. Choose constants to satisfy the inequality:  
   `c‚ÇÅ = 1`, `c‚ÇÇ = 2`, and `n‚ÇÄ = 1`.

Thus, for `n ‚â• 1`, the inequality holds true. Therefore:  
`2n = Œò(n)`

### Brief Overview of Little-o and Little-Omega Notation

#### Little-o Notation (`o`)
- **Definition**: Little-o describes an **upper bound** that is not tight. It means `f(n)` grows strictly slower than `g(n)` as `n` becomes very large.
- **Key Idea**: `f(n)` approaches 0 relative to `g(n)` as `n` increases.
- **Example**: `f(n) = n` is `o(n¬≤)` because `n` grows much slower than `n¬≤`.

#### Little-Omega Notation (`œâ`)
- **Definition**: Little-omega describes a **lower bound** that is not tight. It means `f(n)` grows strictly faster than `g(n)` as `n` becomes very large.
- **Key Idea**: `f(n)` grows infinitely larger compared to `g(n)` as `n` increases.
- **Example**: `f(n) = n¬≤` is `œâ(n)` because `n¬≤` grows much faster than `n`.

---

## Complexity Calculations

Some sample questions below:

Computer solves problem of complexity **T(n) ‚àà ùú£ (n3)** & size **n = 1000** in 1 minute. What size would be solved in 1 hour?

1. Simplify the equation:
`(ùëê ‚àó (100)^3)/(ùëê ‚àó (ùë•)^3) = 1/60`
2. Solve for **x**:
`(ùë•)^3 = 60 * (100)^3`
3. Calculate **x**
`x = 3914`

#### Conclusion: 
The computer can solve a problem of size **x = 3914** in 1 hour. Note that **x ‚àà ‚Ñï** (it must be a natural number).

## Sorting Algorithms

### 1. Bubble Sort
Repeatedly swaps adjacent elements that are out of order.  
- **Best Case**: `O(n)`
- **Worst Case**: `O(n^2)`
- **Space Complexity**: `O(1)`

---

### 2. Selection Sort

Finds the smallest element and places it in the correct position.  

---

### Steps:
- Orders a list of values by repeatedly putting the smallest or largest unplaced value into its final position.
  - Look through the list to find the smallest value.
  - Swap it so that it is at index 0.
  - Look through the list to find the second-smallest value.
  - Swap it so that it is at index 1.
  - Repeat until all values are in their proper places.

---

![Selection Sort Example](/week2-complexity/images/selectionsort.png)

- **Best Case**: `O(n^2)`  
- **Worst Case**: `O(n^2)`  
- **Space Complexity**: `O(1)`

---

### 3. Insertion Sort
Builds a sorted sub-array by repeatedly inserting elements in the correct position.  
- **Best Case**: `O(n)`
- **Worst Case**: `O(n^2)`
- **Space Complexity**: `O(1)`

---

### 4. Merge Sort
Uses the "divide and conquer" strategy to split the array, sort each half, and merge them.  

---

![Merge Sort Example](/week2-complexity/images/mergesort.png)

- **Best Case**: `O(n log n)`
- **Worst Case**: `O(n log n)`
- **Space Complexity**: `O(n)`

---

### 5. Quick Sort
Recursively partitions the array based on a pivot element.  
- **Best Case**: `O(n log n)`
- **Worst Case**: `O(n^2)` (happens when pivot selection is poor)
- **Space Complexity**: `O(n)`

---

## Summary of Sorting Complexities

| **Algorithm**     | **Best Case**     | **Worst Case**    | **Space Complexity** |
|--------------------|-------------------|-------------------|-----------------------|
| **Bubble Sort**    | `Œ©(n)`           | `O(n^2)`          | `O(1)`               |
| **Selection Sort** | `Œ©(n^2)`         | `O(n^2)`          | `O(1)`               |
| **Insertion Sort** | `Œ©(n)`           | `O(n^2)`          | `O(1)`               |
| **Merge Sort**     | `Œ©(n log n)`     | `O(n log n)`      | `O(n)`               |
| **Quick Sort**     | `Œ©(n log n)`     | `O(n^2)`          | `O(n)`               |

## Resources

### Merge Sort:
- [Time and Space Complexity Analysis of Merge Sort](https://www.geeksforgeeks.org/time-and-space-complexity-analysis-of-merge-sort/)

### Time Complexity of Sorting Algorithms:
- [GeeksforGeeks: Time Complexities of All Sorting Algorithms](https://www.geeksforgeeks.org/time-complexities-of-all-sorting-algorithms/)
- [Board Infinity: Time Complexity of Sorting Algorithms](https://www.boardinfinity.com/blog/time-complexity-of-sorting-algorithms/)
